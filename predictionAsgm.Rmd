---
title: "Practical Machine Learning"
author: "Yuani"
date: "19 November 2015"
output: html_document
---

##1. Download Data
The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har.
When importing data, all empty cells were replaced in "NA".str

```{r}
setwd("C:\\Users\\ychen06\\Documents\\IDA MOOC\\Git\\PracticalMachineLearning")
data_training<-read.csv("pml-training.csv",na.strings=c(""," ","NA"))
data_testing<-read.csv("pml-testing.csv",na.strings=c(""," ","NA"))
```

##Clean Dataset
There are 160 different variables in the datasets provided. However, not all variables can be used in developing  our predictions as the entire variable consist of null values.

We first clean the data set to select only variables with values that can be used for analysis.Columns with complete "NA" or blanks are removed.

We also generated a second table consisting of all the predictor variables and the final desired predicted value "classe". These variables were selected based on the goal of using data from accelerometers on the bell, forearm, arm and dumbbell.

```{r}
data_training_cleaned<-data_training[ , colSums(is.na(data_training))== 0 ]
data_predictor<- data_training_cleaned[,grepl("belt|forearm|arm|dumbbell|classe",names(data_training_cleaned))] 

data_testing_cleaned<-data_testing[ , colSums(is.na(data_testing))== 0 ]
data_testing_cleaned<- data_testing_cleaned[,grepl("belt|forearm|arm|dumbbell|classe",names(data_testing_cleaned))] 
```

This results in 53 variables including "classe" which is our predcted outcome.

```{r}
names(data_predictor)
```

The predicted outcomes in our training set is distributed as follow:
```{r}
summary(data_predictor$classe)
```

## Prediction Modeling
To create a check for our prediction model, we split the training set into 10 parts, 9 for training and 1 for cross validation. We first train a model using 9 folds, and test it on the 10th fold to check for accuracy.

```{r}
library(caret)
library(rpart)
```

```{r}
inTrain <- createDataPartition(y=data_predictor$classe, p=0.7, list=FALSE)
train_trainingset <- data_predictor[inTrain,]
train_testingset<- data_predictor[-inTrain,]
```

```{r}
set.seed(1212)
```

```{r}
#Decision Tree Method
modFit<-train(classe ~ . ,method="rpart",data=train_trainingset)
print(modFit$finalModel)
library(rattle)
rattle()
fancyRpartPlot(modFit$finalModel)
prediction_results_tree<-predict(modFit,newdata=train_testingset)
cm1<-confusionMatrix(prediction_results_tree, train_testingset$classe)

```

```{r}
#Random Forest Method
library(randomForest)
modFit2<-randomForest(classe~., data=train_trainingset, ntree = 500)
print(modFit2$finalModel)
prediction_results_rf<-predict(modFit2,newdata=train_testingset)
cm2<-confusionMatrix(prediction_results_rf, train_testingset$classe)

```

##Selection Prediction Model
Comparing the level of specificity and sensitivty bewteen the 2 models, the random forest shows better results, with an accuracy of 99%. We will therefore select the random forest model on the testing data set to predict the 'classe'.

```{r}
print(cm1)
print(cm2)
```

##Prediction Results

```{r}
#model used
print(modFit2$finalModel)
prediction_results_final<-predict(modFit2,newdata=data_testing_cleaned)
final_table<-data_testing_cleaned
final_table$Prediction<-prediction_results_final
final_table
```

